# <center>Описание проекта DATATON_22_boring-wozniak</center>

# Цель проекта

Мы хотим собрать информацию с разных публичных источников о вакансиях в сфере IT, связанных с наукой о данных. Основным географическим регионом будут страны ЕАС. Собирать будем вакансии по отпределенным тегам. 
Данный проект является первым шагом в работе с данными и может расширяться как географически, так и функционально:

* мы сможем добавть больше регионов
* мы сможем смотреть динамику рынка вакансий, в дальнейшем делая срезы рынка из используемых источников с определенной периодичностью
* мы сможем подключить базу данных для хранения информации

# Этапы проекта:

1. Сбор инфорамции о вакансиях с публичных сайтов по тегам [Data Science, Data Analyst, Data Engineer, Machine Learning, MLOps, Аналитик данных, Инженер по данным]. Наше решение - мы собираем максимально возможное количество данных, чтобы потом решить какие из них мы будем использовать в объединенном датасете на постоянной основе. Основные источники наших данных:

    * сайт [career.habr.com](https://career.habr.com) - сайт, который помогает найти работу мечты в IT. Сайт содержит более 3 340 вакансий. Для его парсинга по тегам использовался вот этот парсер [parser](https://github.com/shakhovak/DATATON_22_boring-wozniak/blob/master/HABR_parser.ipynb). С него мы получили 2 702 вакансии для последующего объединения.

    * сайт [superjob.ru](https://www.superjob.ru/) - это лучшие предложения высокооплачиваемой работы от российских и  зарубежных компаний. Собрали 20 вакансий.

    * сайт [rabota.ru](https://www.rabota.ru/) - найдем работу за вас. Для сбора данных использовался метод скраппинга и удалось собрать 300 вакансий. Для работы использовался парсер ссылка.

    * [Telegram канал](https://t.me/datasciencejobs) - лучшие вакансии по Data Science, ML, CV, AI ... Для сборки использовался скраппинг на основе Selenium, доступный по ссылке. С него мы получили несколько сотен вакансий.

    * сайт [hh.ru](https://hh.ru) - один из самых популярных источников вакансий. Мы использовали API для этого сайта и с помощью парсера ссылка получили более 1000 вакансий.

2. Анализ собранных данных. На этом этапе мы смотрим, какие данные нам удалось собрать, анализируем их характеристики и принимаем решение, какие из них остатся в объединенном датасете. 

3. Анализируем сводный датасет и выводим некоторые характеристики рынка вакансий в регионе.