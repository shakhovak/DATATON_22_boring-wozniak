# <center>Описание проекта DATATON_22_boring-wozniak</center>

# Цель проекта

Мы хотим собрать информацию с разных публичных источников о вакансиях в сфере IT, связанных с наукой о данных. Основным географическим регионом будут страны ЕАС. Собирать будем вакансии по отпределенным тегам. 
Данный проект является первым шагом в работе с данными и может расширяться как географически, так и функционально:

* мы сможем добавть больше регионов
* мы сможем смотреть динамику рынка вакансий, в дальнейшем делая срезы рынка из используемых источников с определенной периодичностью
* мы сможем подключить базу данных для хранения информации

# Этапы проекта:

1. Сбор инфорамции о вакансиях с публичных сайтов по тегам [Data Science, Data Analyst, Data Engineer, Machine Learning, MLOps, Аналитик данных, Инженер по данным]. Наше решение - мы собираем максимально возможное количество данных, чтобы потом решить какие из них мы будем использовать в объединенном датасете на постоянной основе. Основные источники наших данных:

    * сайт [career.habr.com](https://career.habr.com) - сайт, который помогает найти работу мечты в IT. Сайт содержит более 3 340 вакансий. Для его парсинга по тегам использовался вот этот парсер [parser](https://github.com/shakhovak/DATATON_22_boring-wozniak/blob/master/parsers_used/HABR_parser.ipynb). С него мы получили более 1 000 вакансии для последующего объединения.

    * сайт [superjob.ru](https://www.superjob.ru/) - это лучшие предложения высокооплачиваемой работы от российских и  зарубежных компаний. Собрали 20 вакансий. Данные собирали с помощью [API](https://github.com/shakhovak/DATATON_22_boring-wozniak/blob/master/parsers_used/superjob_get_data.ipynb) для этого сайта.

    * сайт [rabota.ru](https://www.rabota.ru) - найдем работу за вас. Для сбора данных мы сделали [пасрер](https://github.com/shakhovak/DATATON_22_boring-wozniak/blob/master/parsers_used/zarpalata_ru_api.ipynb) этого сайта на базе API тестовой и продуктивной сред. Анализ бесплатной версии API показал, что для задач дататона не получится собрать полезные данные, так как API позволяет работать с ваканиями только за последний месяц и среди них очень вакансий, связанных с ML & DS. Результаты работы приложены в виде ноутбука к проекту на случай, если появятся задачи, которые сможет решить имеющийся API.

    * [Telegram канал](https://t.me/datasciencejobs) - лучшие вакансии по Data Science, ML, CV, AI ... Для сборки использовался скраппинг на основе Selenium с помощью вот этого [парсера](https://github.com/shakhovak/DATATON_22_boring-wozniak/blob/master/parsers_used/telegram-scrapper.ipynb). С него мы получили несколько более 800 вакансий.

    * сайт [hh.ru](https://hh.ru) - один из самых популярных источников вакансий. Мы использовали API для этого сайта и с помощью парсера получили почти 1000 вакансий.

2. Анализ собранных данных. На этом этапе мы смотрим, какие данные нам удалось собрать, анализируем их характеристики и принимаем решение, какие из них остатся в объединенном датасете. 

    Мы проанализровали данные из разных источников и решили оставить следующие характеристики для объединения:

    * company - работодатель
    * poistion - название позиции
    * location - место работы
    * format - удаленный, в офисе или гибридный
    * salary - уровень заработной платы
    * schedule - график работы
    * level - уровень потенциального сотрудника 
    * techstack - ключевые навыки
    * date_publish - дата публикации
    * source_id - источник информации

По итогам объединения получился датасет в ХХ строк ссылка

3. Анализируем сводный датасет и выводим некоторые характеристики рынка вакансий в регионе.